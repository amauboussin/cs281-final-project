{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from load import all_subreddits_data, tv_subreddits_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# data is of the form {class_label: list of documents}\n",
    "all_data = all_subreddits_data()\n",
    "tv_data = tv_subreddits_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tt_split(data, test_size=.1):\n",
    "    \"\"\"Splits a dictionary {class_label: list of documents}\"\"\"\n",
    "    \"\"\"into two dictionaries of the same shape\"\"\"\n",
    "    train_data = {}; test_data = {}\n",
    "    for label, docs in data.iteritems():\n",
    "        train, test = train_test_split(docs, test_size=test_size)\n",
    "        train_data[label] = train\n",
    "        test_data[label] = test\n",
    "    return train_data, test_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 1, 'c': 0, 'd': 3, 'e': 2, 'f': 5, 'g': 4}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_to_vocab(class_tokens):\n",
    "    \"\"\"{class_label : list of tokenized documents} -> vocab\"\"\"\n",
    "    vocab = set([])\n",
    "    for _class, tokenized_docs in class_tokens.iteritems():\n",
    "        for d in tokenized_docs:\n",
    "            vocab = vocab.union(set(d))\n",
    "    return {word: i for i, word in enumerate(vocab)}\n",
    "        \n",
    "\n",
    "def word_tokenize_doc(doc):\n",
    "    \"\"\"Word tokenize a single document\"\"\"\n",
    "    to_remove = set(['http', 'faq', 'https', 'amp','source', 'deletion', 'sfw',\n",
    "              'nsfw', 'gt', 'gon', 'na', 'delete', 'comment', 'profile'])\n",
    "    def _filter(w):\n",
    "        return all([w.isalnum(), w not in stopwords.words('english'), w not in to_remove])\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = filter(_filter, tokens)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_all_words(data):\n",
    "    \"\"\"basic get_tokens method\"\"\"\n",
    "    \"\"\"{class_label: list of documents} ->\"\"\" \n",
    "    \"\"\"{class_label : list of tokenized documents}\"\"\"\n",
    "    for c, docs in data.iteritems():\n",
    "        data[c] = map(word_tokenize_doc, docs)\n",
    "    return data\n",
    "\n",
    "test_data = {\n",
    "    'c1': ['aa bb c', 'b d'],\n",
    "    'c2': ['d e f', 'e g']\n",
    "}\n",
    "test_tokens = tokenize_all_words(test_data)\n",
    "tokens_to_vocab(test_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lda_topics(train_tokens, vocab):\n",
    "    \"\"\"{class_label: list of tokenized docs} ->\"\"\"\n",
    "    \"\"\"{class_label: list of topic distributions from LDA\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(data, get_tokens=tokenize_all_words,\n",
    "              get_topics = get_lda_topics):\n",
    "    \"\"\"Vectorizes, topic models, classifies, and returns score\"\"\"\n",
    "    train, test = tt_split(data)\n",
    "    train_tokens = get_tokens(train)\n",
    "    vocab = tokens_to_vocab(train_tokens)\n",
    "    class_topic_dists = get_topics\n",
    "    return vocab\n",
    "    \n",
    "    # counts = vectorize(train, vocab)\n",
    "\n",
    "vocab = run_model(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['A', 'a', 'c', 'e', 'd', '0', 'i', 'm', 'l', 'n', 'p', 's', '2', '7', 'v'])\n"
     ]
    }
   ],
   "source": [
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
