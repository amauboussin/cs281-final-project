{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"axes.spines.right\" on line 210 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.flierprops.linewidth\" on line 321 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"contour.corner_mask\" on line 306 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.vertical\" on line 339 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.meanprops.color\" on line 327 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.bootstrap\" on line 312 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"ytick.minor.visible\" on line 230 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.capprops.linestyle\" on line 317 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"legend.facecolor\" on line 266 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.grid.axis\" on line 173 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.patchartist\" on line 334 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1039: UserWarning: Bad val \"None\" on line #264\n",
      "\t\"legend.framealpha    : None    # opacity of of legend frame\n",
      "\"\n",
      "\tin file \"//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle\"\n",
      "\tKey legend.framealpha: Could not convert \"None\" to float\n",
      "  (val, error_details, msg))\n",
      "\n",
      "Bad key \"boxplot.meanline\" on line 326 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.flierprops.color\" on line 319 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.spines.left\" on line 209 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.whiskerprops.linestyle\" on line 341 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.capprops.color\" on line 316 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.medianprops.linestyle\" on line 331 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.flierprops.markeredgecolor\" on line 323 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.spines.top\" on line 211 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"legend.edgecolor\" on line 267 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.spines.bottom\" on line 208 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.prop_cycle\" on line 201 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.whiskerprops.linewidth\" on line 342 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.medianprops.linewidth\" on line 332 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.showmeans\" on line 338 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.meanprops.linestyle\" on line 328 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.capprops.linewidth\" on line 318 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.boxprops.linewidth\" on line 315 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"animation.html\" on line 492 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.showcaps\" on line 336 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.meanprops.linewidth\" on line 329 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"figure.titlesize\" on line 273 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"xtick.minor.visible\" on line 219 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.boxprops.color\" on line 313 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.flierprops.markerfacecolor\" on line 324 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.notch\" on line 333 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.boxprops.linestyle\" on line 314 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"figure.titleweight\" on line 274 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"markers.fillstyle\" on line 20 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.labelpad\" on line 177 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.flierprops.marker\" on line 322 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.medianprops.color\" on line 330 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.showfliers\" on line 337 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.flierprops.markersize\" on line 325 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.whiskerprops.color\" on line 340 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.flierprops.linestyle\" on line 320 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.whiskers\" on line 343 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"errorbar.capsize\" on line 309 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"image.composite_image\" on line 302 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"boxplot.showbox\" on line 335 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/classic.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.prop_cycle\" on line 2 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/seaborn-bright.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.prop_cycle\" on line 2 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/seaborn-colorblind.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.prop_cycle\" on line 2 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/seaborn-dark-palette.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.prop_cycle\" on line 2 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/seaborn-deep.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.prop_cycle\" on line 2 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/seaborn-muted.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n",
      "\n",
      "Bad key \"axes.prop_cycle\" on line 2 in\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/mpl-data/stylelib/seaborn-pastel.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://matplotlib.sf.net/_static/matplotlibrc or from the matplotlib source\n",
      "distribution\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import gensim as gs\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lda\n",
    "from load import all_subreddits_data, tv_subreddits_data\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4a62634f7514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data is of the form {class_label: list of documents}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_subreddits_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# tv_data = tv_subreddits_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andrew/Desktop/CS281/cs281-final-project/load.pyc\u001b[0m in \u001b[0;36mall_subreddits_data\u001b[0;34m(n, per_subreddit, min_comments)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mall_subreddits_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_subreddit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_comments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0msubreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_subreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mk_mean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andrew/Desktop/CS281/cs281-final-project/load.pyc\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((f,))\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mall_subreddits_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_subreddit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_comments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0msubreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_subreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mk_mean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andrew/Desktop/CS281/cs281-final-project/load.pyc\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/unicodecsv/__init__.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         result = dict((uni_key, row[str_key]) for (str_key, uni_key) in\n\u001b[1;32m    190\u001b[0m                       izip(self.fieldnames, self.unicode_fieldnames))\n",
      "\u001b[0;32m//anaconda/lib/python2.7/csv.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# Used only for its side effect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/unicodecsv/__init__.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0municode_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         return [(value if isinstance(value, float_) else\n\u001b[0;32m--> 112\u001b[0;31m                  unicode_(value, encoding, encoding_errors)) for value in row]\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# data is of the form {class_label: list of documents}\n",
    "all_data = all_subreddits_data(30, 500, 10)\n",
    "# tv_data = tv_subreddits_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 486,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(len, all_data.values())\n",
    "# all_data.keys()\n",
    "# all_data2 = {'buildapc': all_data['buildapc'], 'anime': all_data['anime']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tt_split(data, test_size=.1):\n",
    "    \"\"\"Splits a dictionary {class_label: list of documents}\"\"\"\n",
    "    \"\"\"into two dictionaries of the same shape\"\"\"\n",
    "    train_data = {}; test_data = {}\n",
    "    for label, docs in data.iteritems():\n",
    "        train, test = train_test_split(docs, test_size=test_size)\n",
    "        train_data[label] = train\n",
    "        test_data[label] = test\n",
    "    return train_data, test_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokens_to_vocab(class_tokens):\n",
    "    \"\"\"{class_label : list of tokenized documents} -> vocab\"\"\"\n",
    "    vocab = set([])\n",
    "    for _class, tokenized_docs in class_tokens.iteritems():\n",
    "        for d in tokenized_docs:\n",
    "            vocab = vocab.union(set(d))\n",
    "    return {word: i for i, word in enumerate(vocab)}\n",
    "        \n",
    "\n",
    "def word_tokenize_doc(doc):\n",
    "    \"\"\"Word tokenize a single document\"\"\"\n",
    "    to_remove = set(['http', 'faq', 'https', 'amp','source', 'deletion', 'sfw',\n",
    "              'nsfw', 'gt', 'gon', 'na', 'delete', 'comment', 'profile'])\n",
    "    def _filter(w):\n",
    "        return all([w.isalnum(), w not in stopwords.words('english'), w not in to_remove])\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = filter(_filter, tokens)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_all_words(data):\n",
    "    \"\"\"basic get_tokens method\"\"\"\n",
    "    \"\"\"{class_label: list of documents} ->\"\"\" \n",
    "    \"\"\"{class_label : list of tokenized documents}\"\"\"\n",
    "    for c, docs in data.iteritems():\n",
    "        data[c] = map(word_tokenize_doc, docs)\n",
    "    return data\n",
    "\n",
    "# test_data = {\n",
    "#     'c1': ['aa bb c', 'b d'],\n",
    "#     'c2': ['d e f', 'e g']\n",
    "# }\n",
    "# test_tokens = tokenize_all_words(test_data)\n",
    "# test_vocab = tokens_to_vocab(test_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_lda_topics(train_tokens, vocab, n_topics=30):\n",
    "#     \"\"\"{class_label: list of tokenized docs} ->\"\"\"\n",
    "#     \"\"\"{class_label: list of topic distributions from LDA\"\"\"\n",
    "#     all_class_tokens = train_tokens.values()\n",
    "#     flattened_classes = [item for sublist in all_class_tokens for item in sublist]\n",
    "#     flattened_documents = [item for sublist in flattened_classes for item in sublist]\n",
    "#     vectorizer = CountVectorizer(min_df=2, vocabulary=vocab)\n",
    "#     X = vectorizer.fit_transform(flattened_documents)\n",
    "#     lda = LatentDirichletAllocation(n_topics=n_topics)\n",
    "#     X_new = lda.fit_transform(X.toarray())\n",
    "#     return X_new\n",
    "\n",
    "def get_hlda_models(train_tokens, vocab, n_topics=40):\n",
    "    models = {}\n",
    "    dictionary = gs.corpora.Dictionary(map(lambda x: [x], vocab.keys()))\n",
    "    for label, docs in train_tokens.items():\n",
    "        corpus = [dictionary.doc2bow(d) for d in docs]\n",
    "        models[label] = gs.models.HdpModel(corpus, dictionary, T=n_topics)\n",
    "    return models, dictionary\n",
    "\n",
    "def get_lda_models(train_tokens, vocab, n_topics=40):\n",
    "    all_models = {}\n",
    "    def fit_model((label, docs)):\n",
    "        model = lda.LDA(n_topics=n_topics, n_iter=200)\n",
    "        vectorizer = CountVectorizer(min_df=2, vocabulary = vocab, stop_words=None)\n",
    "        X = vectorizer.fit_transform(map(lambda s: ' '.join(s), docs))\n",
    "        model.fit(X)\n",
    "        all_models[label] = model\n",
    "        print 'done fitting for ', label\n",
    "    map(fit_model, train_tokens.items())\n",
    "    return all_models\n",
    "\n",
    "\n",
    "def hlda_pred(models, dictionary, doc):\n",
    "    corpus = [dictionary.doc2bow(word_tokenize_doc(doc))]\n",
    "    label_score = []\n",
    "    for label, hdp in models.iteritems():\n",
    "        label_score.append((label, hdp.evaluate_test_corpus(corpus)))\n",
    "    return max(label_score, key = lambda x:x[1])[0]\n",
    "\n",
    "def lda_pred(models, vocab, doc):\n",
    "    \"\"\"Get a class prediction for a document \"\"\"\n",
    "    tokenized = word_tokenize_doc(doc)\n",
    "    vectorizer = CountVectorizer(min_df=1, vocabulary = vocab, stop_words=None)\n",
    "    X = vectorizer.fit_transform([' '.join(tokenized)])\n",
    "    label_score = []\n",
    "    for label, model in models.iteritems():\n",
    "        n_topics = len(model.components_)\n",
    "        topic_dist = model.transform(X)\n",
    "        log_likelihood = 0\n",
    "        for token in tokenized:\n",
    "            if token in vocab:\n",
    "                max_likelihood = -1 * 10 ** 8\n",
    "                for topic in range(n_topics):\n",
    "                    ll = np.log(model.components_[topic][vocab[token]]) + np.log(topic_dist[0][topic])\n",
    "                    max_likelihood = max_likelihood if max_likelihood > ll else ll\n",
    "                log_likelihood += max_likelihood\n",
    "        label_score.append((label, log_likelihood))\n",
    "    return max(label_score, key = lambda x:x[1])[0]\n",
    "# get_lda_topics(test_tokens, test_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done splitting\n",
      "done tokenizing\n"
     ]
    }
   ],
   "source": [
    "# def run_model(data, get_tokens=tokenize_all_words,\n",
    "#               get_models = get_lda_models):\n",
    "#     \"\"\"Vectorizes, topic models, classifies, and returns score\"\"\"\n",
    "data = all_data\n",
    "get_tokens = tokenize_all_words\n",
    "get_models = get_lda_models\n",
    "\n",
    "train, test = tt_split(data)\n",
    "print 'done splitting'\n",
    "train_tokens = get_tokens(train)\n",
    "print 'done tokenizing'\n",
    "vocab = tokens_to_vocab(train_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lda_models = get_lda_models(train_tokens, vocab)\n",
    "\n",
    "# dump \n",
    "pickle_filepath = 'cache/data.pickle'\n",
    "\n",
    "def dump():\n",
    "    with open(pickle_filepath, 'w') as wfile:\n",
    "        pickle.dump( (train, test, train_tokens, vocab), wfile)\n",
    "\n",
    "def load():\n",
    "    with open(pickle_filepath, 'r') as rfile:\n",
    "        train, test, train_tokens, vocab = pickle.load(rfile)\n",
    "    return train, test, train_tokens, vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, train_tokens, vocab = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct = 0\n",
    "# total = 0\n",
    "# for label, docset in test.iteritems():\n",
    "#     for doc in docset:\n",
    "#         total += 1\n",
    "#         if lda_pred(lda_models, vocab, doc) == label:\n",
    "#             correct += 1\n",
    "# print correct, total\n",
    "from collections import defaultdict\n",
    "lookup = {}\n",
    "words = set()\n",
    "for label, docs in train_tokens.iteritems():\n",
    "    counts = defaultdict(int)\n",
    "    for d in docs:\n",
    "        for t in d:\n",
    "            counts[t] += 1\n",
    "            words.add(t)\n",
    "    lookup[label] = counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import dirichlet\n",
    "def normalize(v):\n",
    "    return v / float(np.sum(v))\n",
    "\n",
    "def score_words(train_tokens):\n",
    "\n",
    "    words = set()\n",
    "    all_counts = {}\n",
    "    for label, docs in train_tokens.iteritems():\n",
    "        counts = defaultdict(int)\n",
    "        for d in docs:\n",
    "            for t in d:\n",
    "                counts[t] += 1\n",
    "                words.add(t)\n",
    "        all_counts[label] = counts\n",
    "\n",
    "\n",
    "    # d filter\n",
    "    word_score = []\n",
    "    for w in words:\n",
    "        get_count = lambda d: d.get(w, 0)\n",
    "\n",
    "        x = normalize(np.array(map(get_count, all_counts.values())))\n",
    "        score = dirichlet.logpdf(normalize(x), np.ones(len(all_counts)) * 2)\n",
    "        word_score.append((w, score))\n",
    "    return word_score\n",
    "\n",
    "\n",
    "def dirichlet_filter(train_tokens, threshold = 20):\n",
    "\n",
    "    word_scores = score_words(train_tokens)\n",
    "\n",
    "    words_to_remove = set(map(lambda x:x[0], filter(lambda x: x[1] > threshold, ws)))\n",
    "\n",
    "    keep_word = lambda w: w not in words_to_remove\n",
    "\n",
    "    removed = 0\n",
    "    total = 0\n",
    "\n",
    "    for label, docs in train_tokens.iteritems():\n",
    "        for i in range(len(docs)):\n",
    "            old_length = len(docs[i])\n",
    "            total += old_length\n",
    "            docs[i] = filter(keep_word, docs[i])\n",
    "            removed += old_length - len(docs[i])\n",
    "        train_tokens[label] = docs\n",
    "\n",
    "    return train_tokens, removed, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/scipy/stats/_multivariate.py:805: RuntimeWarning: divide by zero encountered in log\n",
      "  return - lnB + np.sum((np.log(x.T) * (alpha - 1)).T, 0)\n"
     ]
    }
   ],
   "source": [
    "# train_tokens, removed, total = dirichlet_filter(train_tokens)\n",
    "word_score = score_words(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forget\n",
      "just\n",
      "yea\n",
      "four\n",
      "ended\n",
      "stayed\n",
      "go\n",
      "hell\n",
      "still\n",
      "hate\n",
      "had\n",
      "cuz\n",
      "better\n",
      "only\n",
      "going\n",
      "pretty\n",
      "sorry\n",
      "might\n",
      "ahh\n",
      "surprised\n",
      "ta\n",
      "good\n",
      "holding\n",
      "yo\n",
      "very\n",
      "looks\n",
      "break\n",
      "worst\n",
      "half\n",
      "not\n",
      "one\n",
      "minute\n",
      "freaking\n",
      "like\n",
      "lost\n",
      "edit\n",
      "always\n",
      "twice\n",
      "bad\n",
      "either\n",
      "went\n",
      "last\n",
      "blew\n",
      "everyone\n",
      "eh\n",
      "absolutely\n",
      "yeah\n",
      "second\n",
      "picked\n",
      "forgot\n",
      "close\n",
      "little\n",
      "surprise\n",
      "really\n",
      "missed\n",
      "god\n",
      "wow\n",
      "wouldnt\n",
      "seriously\n",
      "goes\n",
      "hilarious\n",
      "got\n",
      "ass\n",
      "kicked\n",
      "never\n",
      "meh\n",
      "let\n",
      "although\n",
      "put\n",
      "post\n",
      "both\n",
      "ol\n",
      "ok\n",
      "thread\n",
      "oh\n",
      "dunno\n",
      "leaves\n",
      "barely\n",
      "swear\n",
      "plus\n",
      "idk\n",
      "threw\n",
      "dis\n",
      "makes\n",
      "thats\n",
      "dont\n",
      "ca\n",
      "two\n",
      "downvote\n",
      "next\n",
      "wan\n",
      "guess\n",
      "yep\n",
      "dude\n",
      "three\n",
      "lol\n",
      "kidding\n",
      "damn\n",
      "too\n",
      "6\n",
      "happy\n",
      "head\n",
      "that\n",
      "huh\n",
      "took\n",
      "back\n",
      "catch\n",
      "probably\n",
      "glad\n",
      "13\n",
      "look\n",
      "gotten\n",
      "straight\n",
      "getting\n",
      "sucks\n",
      "see\n",
      "yup\n",
      "gave\n",
      "didnt\n",
      "bet\n",
      "gone\n",
      "crazy\n",
      "give\n",
      "ai\n",
      "ah\n",
      "turned\n",
      "confused\n",
      "sad\n",
      "shame\n",
      "cant\n",
      "someone\n",
      "im\n",
      "something\n",
      "ready\n",
      "id\n",
      "shit\n",
      "thought\n",
      "nonetheless\n",
      "nah\n",
      "same\n",
      "ya\n",
      "amazing\n",
      "sick\n",
      "9\n",
      "gets\n",
      "nice\n",
      "badly\n",
      "sure\n",
      "okay\n",
      "hey\n",
      "though\n",
      "haha\n",
      "stay\n",
      "fell\n",
      "mad\n",
      "kinda\n",
      "coming\n",
      "awful\n",
      "suck\n",
      "totally\n",
      "man\n",
      "a\n",
      "maybe\n",
      "no\n",
      "well\n",
      "face\n",
      "looked\n",
      "definitely\n",
      "honestly\n",
      "alright\n",
      "five\n",
      "away\n"
     ]
    }
   ],
   "source": [
    "from string import lower\n",
    "for w in set(map(lower, map(lambda x: x[0], sorted(word_score, key=lambda x: x[1], reverse=True)[:200]))):\n",
    "    print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243112\n"
     ]
    }
   ],
   "source": [
    "# print sorted(ws, key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "# plt.hist(map(lambda x:x[1], ws))\n",
    "# plt.show()\n",
    "words = set()\n",
    "for label, docs in train_tokens.iteritems():\n",
    "    counts = defaultdict(int)\n",
    "    for d in docs:\n",
    "        for t in d:\n",
    "            words.add(t)\n",
    "print len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
      "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 1499\n"
     ]
    }
   ],
   "source": [
    "hlda_models, dictionary = get_hlda_models(train_tokens, vocab, 40)\n",
    "correct = 0\n",
    "total = 0\n",
    "for label, docset in test.iteritems():\n",
    "    for doc in docset:\n",
    "        total += 1\n",
    "        if hlda_pred(hlda_models, dictionary, doc) == label:\n",
    "            correct += 1\n",
    "print correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map(lambda (l, t): map(lambda x: x[0], t), hlda_models['aww'].show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print pred(Out[30], vocab, 'qi')\n",
    "# print pred(Out[30], vocab, 'gold')\n",
    "hlda_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    }
   ],
   "source": [
    "lda_models = get_lda_models(train_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for label, docset in test.iteritems():\n",
    "    for doc in docset:\n",
    "        total += 1\n",
    "        if lda_pred(lda_models, vocab, doc) == label:\n",
    "            correct += 1\n",
    "print correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
