{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import gensim as gs\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lda\n",
    "from load import all_subreddits_data, tv_subreddits_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4a62634f7514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data is of the form {class_label: list of documents}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_subreddits_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# tv_data = tv_subreddits_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andrew/Desktop/CS281/cs281-final-project/load.pyc\u001b[0m in \u001b[0;36mall_subreddits_data\u001b[0;34m(n, per_subreddit, min_comments)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mall_subreddits_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_subreddit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_comments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0msubreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_subreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mk_mean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andrew/Desktop/CS281/cs281-final-project/load.pyc\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((f,))\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mall_subreddits_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_subreddit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_comments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0msubreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_subreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mk_mean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Andrew/Desktop/CS281/cs281-final-project/load.pyc\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/unicodecsv/__init__.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         result = dict((uni_key, row[str_key]) for (str_key, uni_key) in\n\u001b[1;32m    190\u001b[0m                       izip(self.fieldnames, self.unicode_fieldnames))\n",
      "\u001b[0;32m//anaconda/lib/python2.7/csv.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# Used only for its side effect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/unicodecsv/__init__.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0municode_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         return [(value if isinstance(value, float_) else\n\u001b[0;32m--> 112\u001b[0;31m                  unicode_(value, encoding, encoding_errors)) for value in row]\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# data is of the form {class_label: list of documents}\n",
    "all_data = all_subreddits_data(30, 500, 10)\n",
    "# tv_data = tv_subreddits_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 486,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(len, all_data.values())\n",
    "# all_data.keys()\n",
    "# all_data2 = {'buildapc': all_data['buildapc'], 'anime': all_data['anime']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tt_split(data, test_size=.1):\n",
    "    \"\"\"Splits a dictionary {class_label: list of documents}\"\"\"\n",
    "    \"\"\"into two dictionaries of the same shape\"\"\"\n",
    "    train_data = {}; test_data = {}\n",
    "    for label, docs in data.iteritems():\n",
    "        train, test = train_test_split(docs, test_size=test_size)\n",
    "        train_data[label] = train\n",
    "        test_data[label] = test\n",
    "    return train_data, test_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokens_to_vocab(class_tokens):\n",
    "    \"\"\"{class_label : list of tokenized documents} -> vocab\"\"\"\n",
    "    vocab = set([])\n",
    "    for _class, tokenized_docs in class_tokens.iteritems():\n",
    "        for d in tokenized_docs:\n",
    "            vocab = vocab.union(set(d))\n",
    "    return {word: i for i, word in enumerate(vocab)}\n",
    "        \n",
    "\n",
    "def word_tokenize_doc(doc):\n",
    "    \"\"\"Word tokenize a single document\"\"\"\n",
    "    to_remove = set(['http', 'faq', 'https', 'amp','source', 'deletion', 'sfw',\n",
    "              'nsfw', 'gt', 'gon', 'na', 'delete', 'comment', 'profile'])\n",
    "    def _filter(w):\n",
    "        return all([w.isalnum(), w not in stopwords.words('english'), w not in to_remove])\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = filter(_filter, tokens)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_all_words(data):\n",
    "    \"\"\"basic get_tokens method\"\"\"\n",
    "    \"\"\"{class_label: list of documents} ->\"\"\" \n",
    "    \"\"\"{class_label : list of tokenized documents}\"\"\"\n",
    "    for c, docs in data.iteritems():\n",
    "        data[c] = map(word_tokenize_doc, docs)\n",
    "    return data\n",
    "\n",
    "# test_data = {\n",
    "#     'c1': ['aa bb c', 'b d'],\n",
    "#     'c2': ['d e f', 'e g']\n",
    "# }\n",
    "# test_tokens = tokenize_all_words(test_data)\n",
    "# test_vocab = tokens_to_vocab(test_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_lda_topics(train_tokens, vocab, n_topics=30):\n",
    "#     \"\"\"{class_label: list of tokenized docs} ->\"\"\"\n",
    "#     \"\"\"{class_label: list of topic distributions from LDA\"\"\"\n",
    "#     all_class_tokens = train_tokens.values()\n",
    "#     flattened_classes = [item for sublist in all_class_tokens for item in sublist]\n",
    "#     flattened_documents = [item for sublist in flattened_classes for item in sublist]\n",
    "#     vectorizer = CountVectorizer(min_df=2, vocabulary=vocab)\n",
    "#     X = vectorizer.fit_transform(flattened_documents)\n",
    "#     lda = LatentDirichletAllocation(n_topics=n_topics)\n",
    "#     X_new = lda.fit_transform(X.toarray())\n",
    "#     return X_new\n",
    "\n",
    "def get_hlda_models(train_tokens, vocab, n_topics=40):\n",
    "    models = {}\n",
    "    dictionary = gs.corpora.Dictionary(map(lambda x: [x], vocab.keys()))\n",
    "    for label, docs in train_tokens.items():\n",
    "        corpus = [dictionary.doc2bow(d) for d in docs]\n",
    "        models[label] = gs.models.HdpModel(corpus, dictionary, T=n_topics)\n",
    "    return models, dictionary\n",
    "\n",
    "def get_lda_models(train_tokens, vocab, n_topics=40):\n",
    "    all_models = {}\n",
    "    def fit_model((label, docs)):\n",
    "        model = lda.LDA(n_topics=n_topics, n_iter=200)\n",
    "        vectorizer = CountVectorizer(min_df=2, vocabulary = vocab, stop_words=None)\n",
    "        X = vectorizer.fit_transform(map(lambda s: ' '.join(s), docs))\n",
    "        model.fit(X)\n",
    "        all_models[label] = model\n",
    "        print 'done fitting for ', label\n",
    "    map(fit_model, train_tokens.items())\n",
    "    return all_models\n",
    "\n",
    "\n",
    "def hlda_pred(models, dictionary, doc):\n",
    "    corpus = [dictionary.doc2bow(word_tokenize_doc(doc))]\n",
    "    label_score = []\n",
    "    for label, hdp in models.iteritems():\n",
    "        label_score.append((label, hdp.evaluate_test_corpus(corpus)))\n",
    "    return max(label_score, key = lambda x:x[1])[0]\n",
    "\n",
    "def lda_pred(models, vocab, doc):\n",
    "    \"\"\"Get a class prediction for a document \"\"\"\n",
    "    tokenized = word_tokenize_doc(doc)\n",
    "    vectorizer = CountVectorizer(min_df=1, vocabulary = vocab, stop_words=None)\n",
    "    X = vectorizer.fit_transform([' '.join(tokenized)])\n",
    "    label_score = []\n",
    "    for label, model in models.iteritems():\n",
    "        n_topics = len(model.components_)\n",
    "        topic_dist = model.transform(X)\n",
    "        log_likelihood = 0\n",
    "        for token in tokenized:\n",
    "            if token in vocab:\n",
    "                max_likelihood = -1 * 10 ** 8\n",
    "                for topic in range(n_topics):\n",
    "                    ll = np.log(model.components_[topic][vocab[token]]) + np.log(topic_dist[0][topic])\n",
    "                    max_likelihood = max_likelihood if max_likelihood > ll else ll\n",
    "                log_likelihood += max_likelihood\n",
    "        label_score.append((label, log_likelihood))\n",
    "    return max(label_score, key = lambda x:x[1])[0]\n",
    "# get_lda_topics(test_tokens, test_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done splitting\n",
      "done tokenizing\n"
     ]
    }
   ],
   "source": [
    "# def run_model(data, get_tokens=tokenize_all_words,\n",
    "#               get_models = get_lda_models):\n",
    "#     \"\"\"Vectorizes, topic models, classifies, and returns score\"\"\"\n",
    "data = all_data\n",
    "get_tokens = tokenize_all_words\n",
    "get_models = get_lda_models\n",
    "\n",
    "train, test = tt_split(data)\n",
    "print 'done splitting'\n",
    "train_tokens = get_tokens(train)\n",
    "print 'done tokenizing'\n",
    "vocab = tokens_to_vocab(train_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lda_models = get_lda_models(train_tokens, vocab)\n",
    "\n",
    "# dump \n",
    "pickle_filepath = 'cache/data.pickle'\n",
    "\n",
    "def dump():\n",
    "    with open(pickle_filepath, 'w') as wfile:\n",
    "        pickle.dump( (train, test, train_tokens, vocab), wfile)\n",
    "\n",
    "def load():\n",
    "    with open(pickle_filepath, 'r') as rfile:\n",
    "        train, test, train_tokens, vocab = pickle.load(rfile)\n",
    "    return train, test, train_tokens, vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, train_tokens, vocab = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct = 0\n",
    "# total = 0\n",
    "# for label, docset in test.iteritems():\n",
    "#     for doc in docset:\n",
    "#         total += 1\n",
    "#         if lda_pred(lda_models, vocab, doc) == label:\n",
    "#             correct += 1\n",
    "# print correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for topics in range(10, 70, 10):    \n",
    "    hlda_models, dictionary = get_hlda_models(train_tokens, vocab, 60)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for label, docset in test.iteritems():\n",
    "        for doc in docset:\n",
    "            total += 1\n",
    "            if hlda_pred(hlda_models, dictionary, doc) == label:\n",
    "                correct += 1\n",
    "    print correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map(lambda (l, t): map(lambda x: x[0], t), hlda_models['aww'].show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print pred(Out[30], vocab, 'qi')\n",
    "# print pred(Out[30], vocab, 'gold')\n",
    "hlda_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    }
   ],
   "source": [
    "lda_models = get_lda_models(train_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for label, docset in test.iteritems():\n",
    "    for doc in docset:\n",
    "        total += 1\n",
    "        if lda_pred(lda_models, vocab, doc) == label:\n",
    "            correct += 1\n",
    "print correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
