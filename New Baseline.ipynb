{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import gensim as gs\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lda\n",
    "from load import all_subreddits_data, tv_subreddits_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "line contains NULL byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e5424c9517e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data is of the form {class_label: list of documents}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_subreddits_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# tv_data = tv_subreddits_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Steven/Documents/cs281-final-project/load.py\u001b[0m in \u001b[0;36mall_subreddits_data\u001b[0;34m(n, min_comments)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mall_subreddits_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_comments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msubreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mfiltered_subreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mk_mean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Steven/Documents/cs281-final-project/load.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((f,))\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mall_subreddits_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_comments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msubreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sr_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mfiltered_subreddits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mk_mean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Steven/Documents/cs281-final-project/load.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/unicodecsv/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csvfile, fieldnames, restkey, restval, dialect, encoding, errors, *args, **kwds)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfieldnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stringify_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         self.unicode_fieldnames = [_unicodify(f, encoding) for f in\n\u001b[0;32m--> 184\u001b[0;31m                                    self.fieldnames]\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_restkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unicodify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/csv.pyc\u001b[0m in \u001b[0;36mfieldnames\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fieldnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fieldnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/unicodecsv/__init__.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mencoding_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding_errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: line contains NULL byte"
     ]
    }
   ],
   "source": [
    "# data is of the form {class_label: list of documents}\n",
    "all_data = all_subreddits_data()\n",
    "# tv_data = tv_subreddits_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buildapc',\n",
       " 'AskWomen',\n",
       " 'aww',\n",
       " 'amiibo',\n",
       " 'AdviceAnimals',\n",
       " '2007scape',\n",
       " 'anime',\n",
       " 'asoiaf',\n",
       " 'AskMen',\n",
       " 'CasualConversation']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['buildapc',\n",
       " 'AskWomen',\n",
       " 'aww',\n",
       " 'amiibo',\n",
       " 'AdviceAnimals',\n",
       " '2007scape',\n",
       " 'anime',\n",
       " 'asoiaf',\n",
       " 'AskMen',\n",
       " 'CasualConversation']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.keys()\n",
    "# all_data2 = {'buildapc': all_data['buildapc'], 'anime': all_data['anime']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tt_split(data, test_size=.1):\n",
    "    \"\"\"Splits a dictionary {class_label: list of documents}\"\"\"\n",
    "    \"\"\"into two dictionaries of the same shape\"\"\"\n",
    "    train_data = {}; test_data = {}\n",
    "    for label, docs in data.iteritems():\n",
    "        train, test = train_test_split(docs, test_size=test_size)\n",
    "        train_data[label] = train\n",
    "        test_data[label] = test\n",
    "    return train_data, test_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokens_to_vocab(class_tokens):\n",
    "    \"\"\"{class_label : list of tokenized documents} -> vocab\"\"\"\n",
    "    vocab = set([])\n",
    "    for _class, tokenized_docs in class_tokens.iteritems():\n",
    "        for d in tokenized_docs:\n",
    "            vocab = vocab.union(set(d))\n",
    "    return {word: i for i, word in enumerate(vocab)}\n",
    "        \n",
    "\n",
    "def word_tokenize_doc(doc):\n",
    "    \"\"\"Word tokenize a single document\"\"\"\n",
    "    to_remove = set(['http', 'faq', 'https', 'amp','source', 'deletion', 'sfw',\n",
    "              'nsfw', 'gt', 'gon', 'na', 'delete', 'comment', 'profile'])\n",
    "    def _filter(w):\n",
    "        return all([w.isalnum(), w not in stopwords.words('english'), w not in to_remove])\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = filter(_filter, tokens)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_all_words(data):\n",
    "    \"\"\"basic get_tokens method\"\"\"\n",
    "    \"\"\"{class_label: list of documents} ->\"\"\" \n",
    "    \"\"\"{class_label : list of tokenized documents}\"\"\"\n",
    "    for c, docs in data.iteritems():\n",
    "        data[c] = map(word_tokenize_doc, docs)\n",
    "    return data\n",
    "\n",
    "# test_data = {\n",
    "#     'c1': ['aa bb c', 'b d'],\n",
    "#     'c2': ['d e f', 'e g']\n",
    "# }\n",
    "# test_tokens = tokenize_all_words(test_data)\n",
    "# test_vocab = tokens_to_vocab(test_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_lda_topics(train_tokens, vocab, n_topics=30):\n",
    "#     \"\"\"{class_label: list of tokenized docs} ->\"\"\"\n",
    "#     \"\"\"{class_label: list of topic distributions from LDA\"\"\"\n",
    "#     all_class_tokens = train_tokens.values()\n",
    "#     flattened_classes = [item for sublist in all_class_tokens for item in sublist]\n",
    "#     flattened_documents = [item for sublist in flattened_classes for item in sublist]\n",
    "#     vectorizer = CountVectorizer(min_df=2, vocabulary=vocab)\n",
    "#     X = vectorizer.fit_transform(flattened_documents)\n",
    "#     lda = LatentDirichletAllocation(n_topics=n_topics)\n",
    "#     X_new = lda.fit_transform(X.toarray())\n",
    "#     return X_new\n",
    "\n",
    "def get_hlda_models(train_tokens, vocab, n_topics=40):\n",
    "    models = {}\n",
    "    dictionary = gs.corpora.Dictionary(map(lambda x: [x], vocab.keys()))\n",
    "    for label, docs in train_tokens.items():\n",
    "        corpus = [dictionary.doc2bow(d) for d in docs]\n",
    "        models[label] = gs.models.HdpModel(corpus, dictionary, T=n_topics)\n",
    "    return models, dictionary\n",
    "\n",
    "def get_lda_models(train_tokens, vocab, n_topics=40):\n",
    "    all_models = {}\n",
    "    def fit_model((label, docs)):\n",
    "        model = lda.LDA(n_topics=n_topics, n_iter=1500)\n",
    "        vectorizer = CountVectorizer(min_df=2, vocabulary = vocab, stop_words=None)\n",
    "        X = vectorizer.fit_transform(map(lambda s: ' '.join(s), docs))\n",
    "        model.fit(X)\n",
    "        all_models[label] = model\n",
    "        print 'done fitting for ', label\n",
    "    map(fit_model, train_tokens.items())\n",
    "    return all_models\n",
    "\n",
    "\n",
    "def hlda_pred(models, dictionary, doc):\n",
    "    corpus = [dictionary.doc2bow(word_tokenize_doc(doc))]\n",
    "    label_score = []\n",
    "    for label, hdp in models.iteritems():\n",
    "        label_score.append((label, hdp.evaluate_test_corpus(corpus)))\n",
    "    return max(label_score, key = lambda x:x[1])[0]\n",
    "\n",
    "def lda_pred(models, vocab, doc):\n",
    "    \"\"\"Get a class prediction for a document \"\"\"\n",
    "    tokenized = word_tokenize_doc(doc)\n",
    "    vectorizer = CountVectorizer(min_df=1, vocabulary = vocab, stop_words=None)\n",
    "    X = vectorizer.fit_transform([' '.join(tokenized)])\n",
    "    label_score = []\n",
    "    for label, model in models.iteritems():\n",
    "        n_topics = len(model.components_)\n",
    "        topic_dist = model.transform(X)\n",
    "        log_likelihood = 0\n",
    "        for token in tokenized:\n",
    "            if token in vocab:\n",
    "                max_likelihood = -1 * 10 ** 8\n",
    "                for topic in range(n_topics):\n",
    "                    ll = np.log(model.components_[topic][vocab[token]]) + np.log(topic_dist[0][topic])\n",
    "                    max_likelihood = max_likelihood if max_likelihood > ll else ll\n",
    "                log_likelihood += max_likelihood\n",
    "        label_score.append((label, log_likelihood))\n",
    "    return max(label_score, key = lambda x:x[1])[0]\n",
    "# get_lda_topics(test_tokens, test_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done splitting\n",
      "done tokenizingdone splitting\n",
      "done tokenizing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def run_model(data, get_tokens=tokenize_all_words,\n",
    "#               get_models = get_lda_models):\n",
    "#     \"\"\"Vectorizes, topic models, classifies, and returns score\"\"\"\n",
    "data = all_data\n",
    "get_tokens = tokenize_all_words\n",
    "get_models = get_lda_models\n",
    "\n",
    "train, test = tt_split(data)\n",
    "print 'done splitting'\n",
    "train_tokens = get_tokens(train)\n",
    "print 'done tokenizing'\n",
    "vocab = tokens_to_vocab(train_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
      "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n"
     ]
    }
   ],
   "source": [
    "hlda_models, dictionary = get_hlda_models(train_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print pred(Out[30], vocab, 'qi')\n",
    "# print pred(Out[30], vocab, 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386 1706\n",
      "1386 1706\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for label, docset in test.iteritems():\n",
    "    for doc in docset:\n",
    "        total += 1\n",
    "        if hlda_pred(hlda_models, dictionary, doc) == label:\n",
    "            correct += 1\n",
    "print correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    }
   ],
   "source": [
    "lda_models = get_lda_models(train_tokens, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for label, docset in test.iteritems():\n",
    "    for doc in docset:\n",
    "        total += 1\n",
    "        if lda_pred(lda_models, vocab, doc) == label:\n",
    "            correct += 1\n",
    "print correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lb(delta, D, L, V):\n",
    "    scale = 1./np.sqrt(D*delta)\n",
    "    factor = np.sqrt(2*L**(-2) + 2*V**(-2))\n",
    "    return scale*factor\n",
    "\n",
    "def get_max_token_id(token_lists):\n",
    "    return max([x[0] for sublist in token_lists for x in sublist])\n",
    "\n",
    "def expand_token(token, V):\n",
    "    vec = np.array([0]*V)\n",
    "    vec[token[0]] = 1\n",
    "    return vec\n",
    "\n",
    "def get_M1(token_lists, V):\n",
    "    tls_np = np.array(token_lists)\n",
    "    D = tls_np.shape[0]\n",
    "    L = tls_np.shape[1]\n",
    "    M1 = np.array([0]*V)\n",
    "    for d in range(D):\n",
    "        for l in range(L):\n",
    "            M1 += expand_token(tls_np[d,l,:], V)\n",
    "    return M1 / (D*L)\n",
    "\n",
    "def get_M2(token_lists, V, M1, alphas):\n",
    "    alpha0 = alphas.sum()\n",
    "    tls_np = np.array(token_lists)\n",
    "    sum_of_outers = np.array(0, shape=(V,V))\n",
    "    for d in range(D):\n",
    "        for l1 in range(L):\n",
    "            for l2 in range(L):\n",
    "                if l1 != l2:\n",
    "                    sum_of_outers += np.outer(expand_token(tls_np[d, l1, :], V), expand_token(tls_np[d, l2, :], V))\n",
    "    M1_outer = alpha0/(alpha0+1)*np.outer(M1, M1)\n",
    "    return sum_of_outers - M1_outer\n",
    "\n",
    "def get_sv1(M2):\n",
    "    return np.linalg.svd(M2, compute_uv=False)[0]\n",
    "\n",
    "def deltaprime(K, d2, d3, V, beta):\n",
    "    numer = np.log(K/d3)*K*(beta + 2*np.log(K/d2))**2\n",
    "    denom = V*beta\n",
    "    return np.sqrt(numer/denom)\n",
    "\n",
    "def get_sigbar1(K, d1, d2, d3, V, beta, alphas):\n",
    "    a_max = max(alphas)\n",
    "    a_0 = alphas.sum()\n",
    "    a_frac = a_max/(a_0*(a_0 + 1))\n",
    "    numer = (1 + deltaprime(K, d2, d3, V, beta))*V*(beta + K*beta**2)\n",
    "    denom = (V*beta - np.sqrt(2*V*beta*np.log(K/d1)))**2\n",
    "    return a_frac*numer/denom\n",
    "\n",
    "def get_ub(K_guess, d1, d2, d3, V, beta, alphas):\n",
    "    func = lambda K : get_sigbar1(K, d1, d2, d3, V, beta, alphas)\n",
    "    try:\n",
    "        return scipy.optimize.fsolve(func, K_guess)\n",
    "    except:\n",
    "        logger.warning(\"Unable to solve for K. Setting upper bound to 1000.\")\n",
    "        return 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
