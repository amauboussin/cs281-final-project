{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import gensim as gs\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lda\n",
    "from load import all_subreddits_data, tv_subreddits_data, author_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data is of the form {class_label: list of documents}\n",
    "all_data = author_data(10)\n",
    "# tv_data = tv_subreddits_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Stacieinhorrorland',\n",
       " u'JustMe80',\n",
       " u'Flowsephine',\n",
       " u'MexicanSpaceProgram',\n",
       " u'MrJacksEnigma',\n",
       " u'Asdyc',\n",
       " u'beauty_and_the_beach',\n",
       " u'without_gravity',\n",
       " u'jojodancer5',\n",
       " u'blamb211',\n",
       " u'-_-Equinox666-_-',\n",
       " u'GustavoFrings',\n",
       " u'diegojones4',\n",
       " u'KubrickIsMyCopilot',\n",
       " u'tfyuhjnbgf',\n",
       " u'suddenweightloss',\n",
       " u'tinyhousebuilder',\n",
       " u'iKnowALotOfStuff',\n",
       " u'sybaritic_footstool',\n",
       " u'Screwj4ck',\n",
       " u'anthonymyers3000',\n",
       " u'Late_Night_Grumbler',\n",
       " u'Springheeljac',\n",
       " u'StiltzkinTheMoogle',\n",
       " u'roguetroll',\n",
       " u'NaturalInclination',\n",
       " u'GeorgeFromManagement',\n",
       " u'Sexyschizophrenic',\n",
       " u'snow_yoshi',\n",
       " u'Megaross',\n",
       " u'CorDeFerrum',\n",
       " u'goodgirl112',\n",
       " u'MyBiologicalRomance',\n",
       " u'iam4real',\n",
       " u'cocofoshosho1122',\n",
       " u'OptimisticRobotLord',\n",
       " u'NakayamaTakayoshi',\n",
       " u'UniversalChairs',\n",
       " u'Universal-Cereal-Bus',\n",
       " u'CAN_ZIGZAG',\n",
       " u'InsertSomeName',\n",
       " u'DefenestratedEgo',\n",
       " u'BlueInventive',\n",
       " u'BiagioLargo',\n",
       " u'Back2Bach',\n",
       " u'KeganRhode',\n",
       " u'crogi',\n",
       " u'silverblaze92']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_data2 = {'buildapc': all_data['buildapc'], 'anime': all_data['anime']}\n",
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tt_split(data, test_size=.1):\n",
    "    \"\"\"Splits a dictionary {class_label: list of documents}\"\"\"\n",
    "    \"\"\"into two dictionaries of the same shape\"\"\"\n",
    "    train_data = {}; test_data = {}\n",
    "    for label, docs in data.iteritems():\n",
    "        train, test = train_test_split(docs, test_size=test_size)\n",
    "        train_data[label] = train\n",
    "        test_data[label] = test\n",
    "    return train_data, test_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokens_to_vocab(class_tokens):\n",
    "    \"\"\"{class_label : list of tokenized documents} -> vocab\"\"\"\n",
    "    vocab = set([])\n",
    "    for _class, tokenized_docs in class_tokens.iteritems():\n",
    "        for d in tokenized_docs:\n",
    "            vocab = vocab.union(set(d))\n",
    "    return {word: i for i, word in enumerate(vocab)}\n",
    "        \n",
    "\n",
    "def word_tokenize_doc(doc):\n",
    "    \"\"\"Word tokenize a single document\"\"\"\n",
    "    to_remove = set(['http', 'faq', 'https', 'amp','source', 'deletion', 'sfw',\n",
    "              'nsfw', 'gt', 'gon', 'na', 'delete', 'comment', 'profile'])\n",
    "    def _filter(w):\n",
    "        return all([w.isalnum(), w not in stopwords.words('english'), w not in to_remove])\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = filter(_filter, tokens)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_all_words(data):\n",
    "    \"\"\"basic get_tokens method\"\"\"\n",
    "    \"\"\"{class_label: list of documents} ->\"\"\" \n",
    "    \"\"\"{class_label : list of tokenized documents}\"\"\"\n",
    "    for c, docs in data.iteritems():\n",
    "        data[c] = map(word_tokenize_doc, docs)\n",
    "    return data\n",
    "\n",
    "# test_data = {\n",
    "#     'c1': ['aa bb c', 'bb dd'],\n",
    "#     'c2': ['dd ee ff', 'ee gg']\n",
    "# }\n",
    "# test_tokens = tokenize_all_words(test_data)\n",
    "# test_vocab = tokens_to_vocab(test_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_lda_topics(train_tokens, vocab, n_topics=30):\n",
    "#     \"\"\"{class_label: list of tokenized docs} ->\"\"\"\n",
    "#     \"\"\"{class_label: list of topic distributions from LDA\"\"\"\n",
    "#     all_class_tokens = train_tokens.values()\n",
    "#     flattened_classes = [item for sublist in all_class_tokens for item in sublist]\n",
    "#     flattened_documents = [item for sublist in flattened_classes for item in sublist]\n",
    "#     vectorizer = CountVectorizer(min_df=2, vocabulary=vocab)\n",
    "#     X = vectorizer.fit_transform(flattened_documents)\n",
    "#     lda = LatentDirichletAllocation(n_topics=n_topics)\n",
    "#     X_new = lda.fit_transform(X.toarray())\n",
    "#     return X_new\n",
    "\n",
    "def get_hlda_models(train_tokens, vocab, n_topics=40):\n",
    "    models = {}\n",
    "    dictionary = gs.corpora.Dictionary(map(lambda x: [x], vocab.keys()))\n",
    "    for label, docs in train_tokens.items():\n",
    "        corpus = [dictionary.doc2bow(d) for d in docs]\n",
    "        models[label] = gs.models.HdpModel(corpus, dictionary, T=n_topics)\n",
    "    return models, dictionary\n",
    "\n",
    "def get_lda_models(train_tokens, vocab, n_topics=40):\n",
    "    all_models = {}\n",
    "    def fit_model((label, docs)):\n",
    "        model = lda.LDA(n_topics=n_topics, n_iter=1500)\n",
    "        vectorizer = CountVectorizer(min_df=2, vocabulary = vocab, stop_words=None)\n",
    "        X = vectorizer.fit_transform(map(lambda s: ' '.join(s), docs))\n",
    "        model.fit(X)\n",
    "        all_models[label] = model\n",
    "        print 'done fitting for ', label\n",
    "    map(fit_model, train_tokens.items())\n",
    "    return all_models\n",
    "\n",
    "\n",
    "def hlda_pred(models, dictionary, doc):\n",
    "    corpus = [dictionary.doc2bow(word_tokenize_doc(doc))]\n",
    "    label_score = []\n",
    "    for label, hdp in models.iteritems():\n",
    "        label_score.append((label, hdp.evaluate_test_corpus(corpus)))\n",
    "    return max(label_score, key = lambda x:x[1])[0]\n",
    "\n",
    "def lda_pred(models, vocab, doc):\n",
    "    \"\"\"Get a class prediction for a document \"\"\"\n",
    "    tokenized = word_tokenize_doc(doc)\n",
    "    vectorizer = CountVectorizer(min_df=1, vocabulary = vocab, stop_words=None)\n",
    "    X = vectorizer.fit_transform([' '.join(tokenized)])\n",
    "    label_score = []\n",
    "    for label, model in models.iteritems():\n",
    "        n_topics = len(model.components_)\n",
    "        topic_dist = model.transform(X)\n",
    "        log_likelihood = 0\n",
    "        for token in tokenized:\n",
    "            if token in vocab:\n",
    "                max_likelihood = -1 * 10 ** 8\n",
    "                for topic in range(n_topics):\n",
    "                    print label, token, topic, model.components_[topic][vocab[token]],(topic_dist[0][topic])\n",
    "                    ll = np.log(model.components_[topic][vocab[token]]) + np.log(topic_dist[0][topic])\n",
    "                    max_likelihood = max_likelihood if max_likelihood > ll else ll\n",
    "                log_likelihood += max_likelihood\n",
    "        label_score.append((label, log_likelihood))\n",
    "    return max(label_score, key = lambda x:x[1])[0]\n",
    "\n",
    "# models = get_lda_models(test_tokens, test_vocab, n_topics=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lda_pred(models, test_vocab, 'aa aa')\n",
    "\n",
    "# models['c2'].topic_word_, test_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done splitting\n",
      "done tokenizing\n"
     ]
    }
   ],
   "source": [
    "# def run_model(data, get_tokens=tokenize_all_words,\n",
    "#               get_models = get_lda_models):\n",
    "#     \"\"\"Vectorizes, topic models, classifies, and returns score\"\"\"\n",
    "data = all_data\n",
    "get_tokens = tokenize_all_words\n",
    "get_models = get_lda_models\n",
    "\n",
    "train, test = tt_split(data)\n",
    "print 'done splitting'\n",
    "train_tokens = get_tokens(train)\n",
    "print 'done tokenizing'\n",
    "vocab = tokens_to_vocab(train_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
      "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
      "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
      "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n",
      "WARNING:gensim.models.hdpmodel:likelihood is decreasing!\n"
     ]
    }
   ],
   "source": [
    "hlda_models, dictionary = get_hlda_models(train_tokens, vocab, n_topics = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 912\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for label, docset in test.iteritems():\n",
    "    for doc in docset:\n",
    "        total += 1\n",
    "        if hlda_pred(hlda_models, dictionary, doc) == label:\n",
    "            correct += 1\n",
    "print correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = [u'StiltzkinTheMoogle',\n",
    "#  u'Stacieinhorrorland',\n",
    "#  u'JustMe80',\n",
    "#  u'Flowsephine',\n",
    "#  u'MexicanSpaceProgram',\n",
    "#  u'MrJacksEnigma',\n",
    "#  u'Asdyc',\n",
    "#  u'beauty_and_the_beach',]\n",
    "# hlda_models.keys()\n",
    "# map(lambda (l, t): map(lambda x: x[0], t), hlda_models['Flowsephine'].show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done fitting for  Stacieinhorrorland\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " JustMe80\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Flowsephine\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MexicanSpaceProgram\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Asdyc\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " without_gravity\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jojodancer5\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " blamb211\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -_-Equinox666-_-\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " diegojones4\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " KubrickIsMyCopilot\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tfyuhjnbgf\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " suddenweightloss\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sybaritic_footstool\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iKnowALotOfStuff\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tinyhousebuilder\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DefenestratedEgo\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Late_Night_Grumbler\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Springheeljac\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anthonymyers3000\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " roguetroll\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UniversalChairs\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NaturalInclination\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GeorgeFromManagement\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sexyschizophrenic\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Universal-Cereal-Bus\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " snow_yoshi\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Megaross\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CorDeFerrum\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " goodgirl112\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MyBiologicalRomance\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " KeganRhode\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iam4real\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cocofoshosho1122\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OptimisticRobotLord\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " silverblaze92\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NakayamaTakayoshi\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " beauty_and_the_beach\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GustavoFrings\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " StiltzkinTheMoogle\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " InsertSomeName\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Screwj4ck\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BlueInventive\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BiagioLargo\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Back2Bach\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CAN_ZIGZAG\n",
      "done fitting for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " crogi\n",
      "done fitting for  MrJacksEnigma\n"
     ]
    }
   ],
   "source": [
    "lda_models = get_lda_models(train_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dc7279d67bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for label, docset in test.iteritems():\n",
    "    for doc in docset:\n",
    "        total += 1\n",
    "        if pred(lda_models, vocab, doc) == label:\n",
    "            correct += 1\n",
    "print correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
